---
title: "ADS 503 Final Project"
author: "Claire Phibbs, Christopher Richardson, Martin Zagari"
date: '2022-06-17'
output:
  pdf_document: default
  word_document: default
---

```{r Library Setup, include=FALSE}
library(caret)
library(DataExplorer)
library(dplyr)
library(gbm)
library(glmnet)
library(ggplot2)
library(lattice)
library(kableExtra)
library(knitr)
library(MASS)
library(pamr)
library(pROC)
library(RANN)
library(randomForest)
library(Rcpp)
library(ROCR)
library(tidyr)
library(tidyverse)
```

# The Data

Our target variable "Risk Performance" along with the first 13 predictor columns
```{r Data view First 13 columms}
file_loc <- '/Volumes/GoogleDrive/My Drive/503/Project 503/Fico Data/heloc_dataset_v1.csv'
heloc <- read.csv(file_loc)

# sort col names for readability purposes
heloc <- heloc[ , order(names(heloc))]
heloc$RiskPerformance <- as.factor(heloc$RiskPerformance)

knitr::kable(heloc[1:4,c(24,1:13)]) %>%
  kableExtra::kable_styling("striped", full_width = F) %>%
  kableExtra::row_spec(0, angle = -90)
```

Predictor columns 14 to 23
```{r Data view First rest columms}
knitr::kable(heloc[1:4,c(24,14:23)]) %>%
  kableExtra::kable_styling("striped", full_width = F) %>%
  kableExtra::row_spec(0, angle = -90)
```

# EDA
```{r DF Introduction, fig.height=3}
DataExplorer::plot_intro(heloc)
```

```{r Outcome Table, fig.height=4}
# bar plot of response variable; RiskPerformance
barplot(table(heloc$RiskPerformance),
        main="Plot of Response Variable: RiskPerformance",
        xlab="RiskPerformance")

table(heloc$RiskPerformance)
```
A 51:47 split!  Nearly a 50:50 balance!

```{r category df build}
# copy heloc data fore explority purposes
xplore_df <- data.frame(heloc)

noY_bool <- names(xplore_df) != 'RiskPerformance'
xplore_df <- data.frame(xplore_df[, noY_bool])

# No credit history
bool1 <- xplore_df == -9
# No activity in the last year
bool2 <- xplore_df == -8
# No soft hits (those with a 0 assigned have had soft hits)
bool3 <- xplore_df == -7

bool0 <- bool1 | bool2 | bool3

table(xplore_df[bool0])
```
Interesting to see that 13,534 values are -9 (no history)
```{r all row -9 count}
xplore_df <- data.frame(heloc)
bool1 <- xplore_df == -9

xplore_df[bool1] <- 0
nines_across_d_board <- xplore_df[rowSums(xplore_df[,noY_bool]) == 0,]
dim(nines_across_d_board)[1]
```

We have 588 rows that are complete -9 across all features.
```{r all -9s outcome}
table(nines_across_d_board$RiskPerformance)
```

Interesting to see the data setregarding -9s across all features has a 55:45 split.  Taking a chance on anything without any background knowledge is a 50/50 split, this was probably intentionally baked into the HELOC data set to skew results in some way.


```{r all row -8 or -7 outcome count}
# the definition of -7 and -8 are similar though not identical
xplore_df <- data.frame(heloc)
bool1 <- xplore_df == -8
bool2 <- xplore_df == -7

xplore_df[bool1] <- NA
xplore_df[bool2] <- NA

inactive_records <- xplore_df[rowSums(is.na(xplore_df)) > 0,]
table(inactive_records$RiskPerformance)
```
As we see, the dataset has a nearly balanced mixture of those with an inactive credit history; though this lens is through a macroscope and not further dividing the data based on certain features which will be shown via histograms and box plots.


```{r all row -8 or -7 count}
bool1 <- inactive_records == NA
inactive_records[bool1] <- 0
inactive_records <- inactive_records[,noY_bool]
inactive_records <- inactive_records[(rowSums(inactive_records)) == 0,]
# dim(inactive_records)[1] 

# unable to figure out how to fix all NA dataframe. 
# Code should return a dataframe dim of 0,23.
```

*Note:  Cell above changes inactive_records returns a dataframe filled with NAs but with the same structure as HELOC.  After further analysis, we concluded there are no records completely filled with -8s or -7s.

This makes sense, for the fact that it is highly likely that people with such records have a mature financial history, while those with a -9 are those starting their financial history; meaning that it is more likely for an individual with a -9 to have -9s accross the board.


## Data Pre-Processing

We are going to preprocess the data to get quality data insights.

```{r pre-processing, fig.height=3}
# -9 = No Credit History
# -8 and -7 = No recent activity
heloc[heloc == -9] <- NA
heloc[heloc == -8] <- NA
heloc[heloc == -7] <- NA
DataExplorer::plot_intro(heloc)
```
It appears that only 76.1% of our data had one or more predictors had a -9,-8,-7.  Thus the following code removes all missing values that span across all columns.

```{r ALL NULL removal, fig.height=3}
# removing missing values from rows that span across all columns (588 values)
heloc_No_NA <- heloc %>% dplyr::filter_at(vars(-RiskPerformance),
                                          any_vars(!is.na(.)))
DataExplorer::plot_intro(heloc_No_NA)
```


```{r Imputation}
# create training indices
set.seed(3)
heloc_training <- caret::createDataPartition(heloc_No_NA$RiskPerformance,
                                             p=0.8,
                                             list=FALSE)

# training/set sets
heloc_train <- heloc_No_NA[heloc_training, ]
heloc_test <- heloc_No_NA[-heloc_training, ]

# knn imputation
heloc_impute <- caret::preProcess(heloc_train,
                                  method = 'knnImpute')

heloc_train <- stats::predict(heloc_impute,
                              newdata=heloc_train)

heloc_test <- stats::predict(heloc_impute,
                             newdata=heloc_test)

# remove highly correlated predictors
high_corr <- caret::findCorrelation(stats::cor(heloc_train[, -24]),
                                    0.85)

# removal of high cor predictors
heloc_train <- heloc_train[, -(high_corr)]
heloc_test <- heloc_test[, -(high_corr)]
names(heloc_test[high_corr])
```

We keep the heloc_train/test dataframes for formula based functions.

```{r train/test split}
no_risk_bool <- names(heloc_train) != 'RiskPerformance'

# x = predictors
heloc_train_x <- heloc_train[,no_risk_bool]
heloc_test_x <- heloc_test[,no_risk_bool]

# y = response/target
heloc_train_y <- heloc_train[,'RiskPerformance']
heloc_test_y <- heloc_test[,'RiskPerformance']
```

## EDA

#### Correlation Plot


```{r Corr Plot}
# correlations
corrplot::corrplot(stats::cor(heloc_train_x),
                   number.cex = 0.5,
                   tl.cex = 0.8)
```
  
  

### Histograms
  
   
   

#### Overall w/out imputation
<br>  
<br>
  
  

```{r Histograms Overall }
# histograms to view predictor variable frequencies 
par(mfrow=c(3,4))
Hmisc::hist.data.frame(heloc[,1:23])
```



```{r Skewness, echo=TRUE}
skewed <- apply(heloc_train_x, 2, moments::skewness)
skewed[skewed > 0.05]
skew_count <- length(skewed[skewed > 0.05])
cat('We have', skew_count,'skewed variables.')
```
  
  
  

#### Hist By Outcome w/ Imputation
  
  
  

```{r Histograms by Outcome, fig.height=3, messages=FALSE, warning=FALSE}
# Deselect Bool Outcome/Response/Target variable
no_risk_bool <- names(heloc_train) != 'RiskPerformance'

# heloc_imputed_full_set <- dplyr::as_tibble(heloc_imputed_full_set)
heloc_ifs_names <- colnames(heloc_train[,no_risk_bool])

# empty list to gather all
plot_list <- list()
for (name in heloc_ifs_names){
  p <- heloc_train %>%
    ggplot( aes(x=heloc_train[,name], fill=RiskPerformance)) +
      geom_histogram( bins=30, color="#e9ecef", alpha=0.6, position = 'identity') +
      scale_fill_manual(values=c("#69b3a2", "#404080")) +
      xlab(name) +
      labs(fill="")
  print(p)
}
```


### BoxPlots

#### Overall


```{r BoxPlot Combined}
# boxplot to view outliers 
boxplot(heloc[, 1:23])
```

  
  
  
#### Box plots By Outcome
  
  
  

```{r BoxPlots by group, fig.height=2.5, messages=FALSE, warning=FALSE}
for (name in heloc_ifs_names){
  # boxplot to view outliers
  p <- heloc_train %>%
          ggplot( aes(x=heloc_train[,name], y=name, fill=RiskPerformance)) +
          geom_boxplot() +
          xlab(name)
  print(p) 
}
```



# Models
With the following models, we are trying to maximize the SPECIFICITY of the models to prevent any non-qualified loanees being presented with a HELOC loan.


Specificity is defined as:
  "The specificity is defined as the rate that nonevent samples are predicted as nonevents" (Kuhn & Johnson, 2013)
  
  
```{r train control setup}
control <- caret::trainControl(method="cv",
                               classProbs=TRUE,
                               savePredictions=TRUE,
                               summaryFunction=twoClassSummary)
```


```{r Helper Functions}
bestIndex <- function(model){
  # returns top ROC value and surrounding indices from model
  highest_score <- max(model$results$ROC)

  # get row index and convert to type Int
  best_index <- rownames(model$results[model$results$ROC == highest_score,])
  best_index <- as.integer(best_index)

  return(best_index)
}

confusionMatrix <- function(testResults.model){
  caret::confusionMatrix(testResults.model,
                       as.factor(testResults$obs),
                       positive="Good")
}

importanceRanker <- function(model_varImp.importance, name){
  # coerce ranking into dataframe structure for manipulation
  df <- data.frame(model_varImp.importance)
  
  df_column_count <- dim(df)[2]
  # if dataframe has 2 columns, reduce to 1
  # NOTE: Values in both columns are the same
  if (df_column_count == 2){
    # grab only 1 column
    df <- df[,1, drop=FALSE]
  }
  
  # overwrite name
  names(df) <- name
  # reverse order ranking (highest num ranked = 1)
  df[,name] <- rank(-df[,name])
  df <- t(df)
  return(df)
}
  
modelScoreBoard <- function(testResults){
  # feed in testResults dataframe and out comes a model scoreboard!
  scoreboard <- data.frame()

  bool <- names(testResults) != 'obs'
  col_names <- colnames(testResults[,bool])
  
  for (colname in col_names){
    
    testResults.model <- testResults[,colname]
    cf <- caret::confusionMatrix(testResults.model,
                                 as.factor(testResults$obs),
                                 positive="Good")
    
    # gather testResults
    acc <- data.frame(metric=cf$overall[1])
    # gather Precision, Sensitivity, Specificity, & F1
    metrics <- list(cf$byClass[c(5,1,2,7)])
    metrics <- data.frame(Metrics=metrics)
    names(metrics) <- 'metric'
    # gather all metrics in 1 df
    metrics <- rbind(acc,metrics)
    names(metrics) <- colname
    
    metrics <- t(metrics)
    scoreboard <- rbind(scoreboard, metrics)
  }
  return(scoreboard)
}

# helper function for roc 
roc_build <- function(model) {
  THE_ROC <- roc(response = model$pred$obs,
                 predictor = model$pred$Bad,
                 levels = rev(levels(model$pred$obs)))
  return(THE_ROC)
}
```

## Discriminant Classification Models

### LDA

```{r LDA}
set.seed(100)
lda_model <- caret::train(x=heloc_train_x,
                          y=heloc_train_y,
                          method="lda",
                          metric="ROC",
                          trControl=control)

lda_modelRoc <- roc_build(lda_model)
lda_model
```

```{r LDA Confusion Matrix}
lda_predictions <- stats::predict(lda_model, heloc_test_x)

# create dataframe to store
testResults <- data.frame(obs=heloc_test_y,
                          lda_model=lda_predictions)

# confusion matrix
confusionMatrix(testResults$lda_model)
```
```{r LDA variable importance}
lda_varImp <- caret::varImp(lda_model, scale=FALSE)
lda_varImpRanks <- importanceRanker(lda_varImp$importance, 'lda')
plot(lda_varImp, top=20)
```

### Logistic Regression


```{r logistic regression}
set.seed(100)
logreg_model <- caret::train(x=heloc_train_x,
                             y=heloc_train_y,
                             method="glm",
                             metric="ROC",
                             trControl=control)

testResults$log_reg_model <- stats::predict(logreg_model, heloc_test_x)
logreg_modelRoc <- roc_build(logreg_model)
logreg_model
```

```{r logistic regression coeffs}
logreg_model$finalModel$coefficients
```

```{r logistic regression confusion matrix}
confusionMatrix(testResults$log_reg_model)
```

```{r logistic regression variable importance}
lr_varImp <- caret::varImp(logreg_model, scale=FALSE)
lr_varImpRanks <- importanceRanker(lr_varImp$importance, 'lr')
plot(lr_varImp, top=20)
```

#### Cost Matrix Threshold Analysis


```{r LR cost matrix threshold anaylsis}
#get raw probs from model
predictions <- predict(logreg_model, heloc_test_x, type = 'prob')
predictions$OBS <- as.factor(heloc_test$RiskPerformance)
predictions <- predictions %>%
  mutate(lr10 = as.factor(if_else(Bad > 0.1, 'Bad', 'Good')))
predictions <- predictions %>% 
  mutate(lr20 = as.factor(if_else(Bad > 0.2, 'Bad', 'Good')))
predictions <- predictions %>% 
  mutate(lr30 = as.factor(if_else(Bad > 0.3, 'Bad', 'Good')))
predictions <- predictions %>% 
  mutate(lr40 = as.factor(if_else(Bad > 0.4, 'Bad', 'Good')))
predictions <- predictions %>% 
  mutate(lr50 = as.factor(if_else(Bad > 0.5, 'Bad', 'Good')))
predictions <- predictions %>% 
  mutate(lr60 = as.factor(if_else(Bad > 0.6, 'Bad', 'Good')))
predictions <- predictions %>% 
  mutate(lr70 = as.factor(if_else(Bad > 0.7, 'Bad', 'Good')))
predictions <- predictions %>% 
  mutate(lr80 = as.factor(if_else(Bad > 0.8, 'Bad', 'Good')))
predictions <- predictions %>% 
  mutate(lr90 = as.factor(if_else(Bad > 0.9, 'Bad', 'Good')))

# cf function 
cost_confusionMatrix <- function(prediction.rate){
  cm <- caret::confusionMatrix(prediction.rate,
                               predictions$OBS,
                               positive = "Bad")
  return(cm)
}

CF10 <- cost_confusionMatrix(predictions$lr10)
CF20 <- cost_confusionMatrix(predictions$lr20)
CF30 <- cost_confusionMatrix(predictions$lr30)
CF40 <- cost_confusionMatrix(predictions$lr40)
CF50 <- cost_confusionMatrix(predictions$lr50)
CF60 <- cost_confusionMatrix(predictions$lr60)
CF70 <- cost_confusionMatrix(predictions$lr70)
CF80 <- cost_confusionMatrix(predictions$lr80)
CF90 <- cost_confusionMatrix(predictions$lr90)

Costs = matrix(c(0,-1000*.85,-60,60), ncol=2, nrow=2)

Prev = matrix(c(9.6/50,9.6/50,2,2), ncol=2, nrow=2)

CF10$table
sum(CF10$table*Costs*Prev)
CF20$table
sum(CF20$table*Costs*Prev)
CF30$table
sum(CF30$table*Costs*Prev)
CF40$table
sum(CF40$table*Costs*Prev)
CF50$table
sum(CF50$table*Costs*Prev)
CF60$table
sum(CF60$table*Costs*Prev)
CF70$table
sum(CF70$table*Costs*Prev)
CF80$table
sum(CF80$table*Costs*Prev)
CF90$table
sum(CF90$table*Costs*Prev)

PlotProf<-data.frame(percent_bad_thresh = c(10,20,30,40,50,60,70,80,90),
                     profit = c(sum(CF10$table*Costs*Prev),
                                sum(CF20$table*Costs*Prev),
                                sum(CF30$table*Costs*Prev),
                                sum(CF40$table*Costs*Prev),
                                sum(CF50$table*Costs*Prev),
                                sum(CF60$table*Costs*Prev),
                                sum(CF70$table*Costs*Prev),
                                sum(CF80$table*Costs*Prev),
                                sum(CF90$table*Costs*Prev)))

ggplot(PlotProf, aes(y=profit, x=percent_bad_thresh)) +
  geom_line(colour = 'red') +
  geom_hline(yintercept=0, linetype='dashed', color='blue')
```



### Penalized Logistic Regression


```{r penalized logistic regression}
set.seed(100)
glmnGrid <- expand.grid(alpha=c(0, 0.1, 0.2, 0.4, 0.6, 0.8, 1),
                        lambda=seq(0.01, 0.2, length=5))

logreg_penalized_model <- caret::train(x=heloc_train_x,
                                       y=heloc_train_y,
                                       method="glmnet",
                                       metric="ROC",
                                       tuneGrid=glmnGrid,
                                       trControl=control)

# bestIndex(logreg_penalized_model)
logreg_penalized_model$results[3:7,1:5]
logreg_penalized_modelRoc <- roc_build(logreg_penalized_model)
```
Based on the best ROC, has the best specificity, our main metric.  This enables us to insure that we only accept best qualified candidates, thus reducing the risk of a loanee defaulting on a $100,000 loan.

```{r penalized LR Confusion Matrix}
# Utilizing Best Model
testResults$logreg_penalized_model <- stats::predict(logreg_penalized_model,
                                                     heloc_test_x)
# confusion matrix
confusionMatrix(testResults$logreg_penalized_model)
```


```{r penalized LR variable importance}
lr_penalized_varImp <- caret::varImp(logreg_penalized_model, scale=FALSE)
lr_pen_varImpRanks <- importanceRanker(lr_penalized_varImp$importance, 'lr_penalized')
plot(lr_penalized_varImp, top=20)
```

## Nonlinear Classification Models

### Flexibble Discriminant Analysis


```{r Flexible Discriminant GridSearch}
# set.seed(100)
# fdaGrid <- expand.grid(degree=c(1,2),
#                        nprune=seq(14, 20, 1))
# 
# set.seed(100)
# fdaModel <- caret::train(x = heloc_train_x,
#                          y = heloc_train_y,
#                          method = "fda",
#                          metric = "ROC",
#                          trControl=control,
#                          tuneGrid=fdaGrid)
# # bestIndex(fdaModel)
# fdaModel$results[1:18,1:5]
```

As we see, the best is that of nprune 16 with a specificity of 72.15% (index = 15).

```{r Flexible Discriminant Tuning}
fdaGrid <- expand.grid(degree=1,
                       nprune=16)

set.seed(100)
fdaModel <- caret::train(x = heloc_train_x,
                         y = heloc_train_y,
                         method = "fda",
                         metric = "ROC",
                         trControl=control,
                         tuneGrid=fdaGrid)

fda_modelRoc <- roc_build(fdaModel)
```

```{r FDA Confusion Matrix}
testResults$fda_model <- predict(fdaModel,
                                 heloc_test_x)
# confusion matrix
confusionMatrix(testResults$fda_model)
```

```{r FDA variable importance}
fda_varImp <- caret::varImp(fdaModel, scale=FALSE)
fda_varImpRanks <- importanceRanker(fda_varImp$importance, 'fda_penalized')
plot(fda_varImp, top=20)
```

### Neural Network


```{r neural network GridSearch}
# set.seed(100)
# nnetGrid <- expand.grid(size = 1:2,
#                         decay = c(0, 0.1, 0.25, 0.5, 0.75, 1))
# 
# nnetModel <- caret::train(x = heloc_train_x,
#                           y = heloc_train_y,
#                           method = "nnet",
#                           tuneGrid = nnetGrid,
#                           metric = "ROC",
#                           trace = FALSE,
#                           maxit = 2000,
#                           trControl = control)
# nnetModel$bestTune$results[1:6,1:5]
```

It appears that the model starts to over fit once the decay goes to 0.1.  The nnet model chose size=2 with decay of 2, with nearly identical ROC, Sensitivity, and Specificity scores and thus size 1 with decay 0 is our choice.

```{r neural network tuning}
set.seed(100)
nnetGrid <- expand.grid(size = 1,
                        decay = 0)

nnetModel <- caret::train(x = heloc_train_x,
                   y = heloc_train_y,
                   method = "nnet",
                   tuneGrid = nnetGrid,
                   metric = "ROC",
                   trace = FALSE,
                   maxit = 2000,
                   trControl = control)

nnet_modelRoc <- roc_build(nnetModel)
```

```{r neural network Confusion Matrix}
testResults$nnet_model <- predict(nnetModel,
                                  heloc_test_x)
# confusion matrix
confusionMatrix(testResults$nnet_model)
```

```{r neural network variable importance}
nn_varImp <- caret::varImp(nnetModel, scale=FALSE)
nn_varImpRanks <- importanceRanker(nn_varImp$importance, 'nn')
plot(nn_varImp, top=20)
```

## Classification Trees

### Boosted Tree


```{r GBM Boosted Tree GridSearch}
# gbmGrid <- expand.grid(interaction.depth =  c(2,3),
#                        n.trees = c(1000,2000,3000,4000), #default val = 1000
#                        shrinkage = c(0.01, 0.1),
#                        n.minobsinnode = c(5,10)) # default val = 10
# set.seed(100)
# gbmModel <- caret::train(x = heloc_train_x,
#                          y = heloc_train_y,
#                          method = "gbm",
#                          tuneGrid = gbmGrid,
#                          verbose = FALSE,
#                          metric = "ROC",
#                          trControl= control)
# gbmModel$results
```

```{r Boosted Tree Model Tuning}
gbmGrid <- expand.grid(interaction.depth = 2,
                       n.trees = 1000,
                       shrinkage = 0.01,
                       n.minobsinnode = 5)
set.seed(100)
gbmModel <- caret::train(x = heloc_train_x,
                         y = heloc_train_y,
                         method = "gbm",
                         tuneGrid = gbmGrid,
                         verbose = FALSE,
                         metric = "ROC",
                         trControl= control)

gbm_modelRoc <- roc_build(gbmModel)
```

```{r Boosted Tree Confusion Matrix}
testResults$gbm_model <- predict(gbmModel,
                                 heloc_test_x)
# confusion matrix
confusionMatrix(testResults$gbm_model)
```

```{r Boosted Tree variable importance}
gbm_varImp <- caret::varImp(gbmModel, scale=FALSE)
gbm_varImpRanks <- importanceRanker(gbm_varImp$importance, 'gbm')
plot(gbm_varImp, top=20)
```

#### GBM with monotonic constraints


```{r GBM_Mono GridSearch}
# gbmGrid <- expand.grid(interaction.depth = c(2,3),
#                        n.trees = c(1000,2000),
#                        shrinkage = c(0.01, 0.1),
#                        n.minobsinnode = c(5,10))
# set.seed(100)
# gbmMono_model <- caret::train(x = heloc_train_x,
#                               y = heloc_train_y,
#                               method = "gbm",
#                               var.monotone = c(-1,-1,-1,-1,-1,-1,-1,-1,
#                                                1,1,1,1,0,0,-1,0,-1,1,
#                                                0,-1,1),
#                               tuneGrid = gbmGrid,
#                               verbose = FALSE,
#                               metric = "ROC",
#                               trControl= control)
# gbmMono_model$results
```

```{r GBM_Mono Model Tune}
gbmGrid <- expand.grid(interaction.depth = 2,
                       n.trees = 1000,
                       shrinkage = 0.01,
                       n.minobsinnode = 5)
set.seed(100)
gbmMono_model <- caret::train(x = heloc_train_x,
                         y = heloc_train_y,
                         var.monotone = c(-1,-1,-1,-1,-1,-1,-1,-1,
                                          1,1,1,1,0,0,-1,0,-1,1,
                                          0,-1,1),
                         method = "gbm",
                         tuneGrid = gbmGrid,
                         verbose = FALSE,
                         metric = "ROC",
                         trControl= control)

gbmMono_modelRoc <- roc_build(gbmMono_model)
```

```{r GBM_Mono Confusion Matrix}
testResults$gbmMono_model <- predict(gbmMono_model,
                                     heloc_test_x)
confusionMatrix(testResults$gbmMono_model)
```

```{r GBM_Mono variable importance}
gmbMono_varImp <- caret::varImp(gbmMono_model, scale=FALSE)
gbmMono_varImpRanks <- importanceRanker(gmbMono_varImp$importance, 'gbmMono')
plot(gmbMono_varImp, top=20)
```

## CART


```{r rpart model build GridSearch}
# set.seed(100)
# rpart_grid <- expand.grid(cp=c(0.0005, 0.001250, 0.0015, 0.00175, 0.002))
# 
# rpart_model <- caret::train(x=heloc_train_x,
#                             y=heloc_train_y,
#                             method="rpart",
#                             metric="ROC",
#                             trControl=control,
#                             tuneGrid = rpart_grid)
# 
# testResults$rpart_model <- predict(rpart_model, heloc_test_x)
# 
# rpart_model
```

As we see, specificity for this model is one of the worst out of all the models we have.

```{r rpart model tuning}
set.seed(100)
rpart_grid <- expand.grid(cp=0.00175)

rpart_model <- caret::train(x=heloc_train_x,
                            y=heloc_train_y,
                            method="rpart",
                            metric="ROC",
                            trControl=control,
                            tuneGrid = rpart_grid)

rpart_modelRoc <- roc_build(rpart_model)
rpart_model
```

```{r rpart confusion matrix}
testResults$rpart_model <- predict(rpart_model, heloc_test_x)

confusionMatrix(testResults$rpart_model)
```

```{r rpart variable importance}
rpart_varImp <- caret::varImp(rpart_model, scale=FALSE)
rpart_varImpRanks <- importanceRanker(rpart_varImp$importance, 'RPart')
plot(rpart_varImp, top=20)
```

## Random Forest


```{r Random Forest GridSearch}
# set.seed(100)
# rf_grid <- expand.grid(mtry=c(5,10,15))
# 
# randomForest_model <- caret::train(x=heloc_train_x,
#                           y=heloc_train_y,
#                           method="rf",
#                           metric="ROC",
#                           trControl=control,
#                           tuneGrid = rf_grid)
# 
# randomForest_model
```

```{r Random Forest Model Tune}
set.seed(100)
rf_grid <- expand.grid(mtry=5)

randomForest_model <- caret::train(x=heloc_train_x,
                                   y=heloc_train_y,
                                   method="rf",
                                   metric="ROC",
                                   trControl=control,
                                   tuneGrid = rf_grid)

randomForest_modelRoc <- roc_build(randomForest_model)
randomForest_model
```


```{r Random Forest confusion matrix}
testResults$randomForest_model <- predict(randomForest_model, heloc_test_x)

confusionMatrix(testResults$randomForest_model)
```

```{r RF variable importance}
rf_varImp <- caret::varImp(randomForest_model, scale=FALSE)
rf_varImpRanks <- importanceRanker(rf_varImp$importance, 'RF')
plot(rf_varImp, top=20)
```

## Results & Discussion

```{r Variable Importance  COMPARISON}
varImpRanks_df <- plyr::rbind.fill.matrix(fda_varImpRanks,gbm_varImpRanks)
# rbind fill removes row names
rownames(varImpRanks_df) <- c('fda','gbm')
# base::rbind preserves row names
varImpRanks_df <- rbind(varImpRanks_df, gbmMono_varImpRanks)
varImpRanks_df <- rbind(varImpRanks_df, lda_varImpRanks)
varImpRanks_df <- rbind(varImpRanks_df, lr_varImpRanks)
varImpRanks_df <- rbind(varImpRanks_df, lr_pen_varImpRanks)
varImpRanks_df <- rbind(varImpRanks_df, nn_varImpRanks)
varImpRanks_df <- rbind(varImpRanks_df, rpart_varImpRanks)
varImpRanks_df <- rbind(varImpRanks_df, rf_varImpRanks)
varImpRanks_df <- data.frame(varImpRanks_df)
# num of columns is hard to print, thus transpose for printing purposes
varImpRanks_df <- t(varImpRanks_df)
knitr::kable(varImpRanks_df) %>%
  kableExtra::kable_styling("striped", full_width = F) %>%
  kableExtra::row_spec(0, angle = -90)
```


```{r ROC COMPARISON}
plot(lda_modelRoc, type='s', col='antiquewhite4', legacy.axes=TRUE)
plot(logreg_modelRoc, type='s', col='aquamarine3', legacy.axes=TRUE, add=TRUE)
plot(logreg_penalized_modelRoc, type='s', col='blue', legacy.axes=TRUE, add=TRUE)
plot(fda_modelRoc, type='s', col='blueviolet', legacy.axes=TRUE, add=TRUE)
plot(nnet_modelRoc, type='s', col='brown', legacy.axes=TRUE, add=TRUE)
plot(gbm_modelRoc, type='s', col='cadetblue', legacy.axes=TRUE, add=TRUE)
plot(gbmMono_modelRoc, type='s', col='red', legacy.axes=TRUE, add=TRUE)
plot(rpart_modelRoc, type='s', col='chartreuse', legacy.axes=TRUE, add=TRUE)
plot(randomForest_modelRoc, type='s', col='cornflowerblue', legacy.axes=TRUE, add=TRUE)

legend_ <- c('LDA', 'LR', 'Penalized LR','FDA', 'NNET','GBM','GBM+mono', 'RPART','RandomForest')
colors_ <-c('antiquewhite4',
            'aquamarine3',
            'blue',
            'blueviolet',
            'brown',
            'cadetblue',
            'red',
            'chartreuse',
            'cornflowerblue')
legend('bottomright', legend=legend_,
       col=colors_, lwd=2)
title(main = 'Compare ROC curves from different models', outer = TRUE)
```


```{r METRIC Comparison}
# gather all model AUCs in 1 list
aucs <- c(lda_modelRoc$auc,
          logreg_modelRoc$auc,
          logreg_penalized_modelRoc$auc,
          fda_modelRoc$auc,
          nnet_modelRoc$auc,
          gbm_modelRoc$auc,
          gbmMono_modelRoc$auc,
          rpart_modelRoc$auc,
          randomForest_modelRoc$auc)

scoreboard <- modelScoreBoard(testResults)
# add AUC list as a column
scoreboard$AUC <- aucs
scoreboard
```


# REFERENCES:

Kuhn, M., & Johnson, K. (2013). Applied Predictive Modeling. New York: Springer.
